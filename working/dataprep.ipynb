{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "086a5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import deepchem as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71fbf72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_categorical(dataframe, cols=None):\n",
    "    \"\"\"\n",
    "    Converts non-numerical categorical values to integers.  This function is most useful for ordinal variables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - dataframe: featurized dataframe\n",
    "    - cols: list of categorical columns to convert to integers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - modified dataframe with user selected columns with categorical string values converted to integer values\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate binary values using get_dummies\n",
    "    if cols is not None:\n",
    "        if type(cols) == str: # allow single columns to be input as strings\n",
    "            cols = [cols]\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        for col in cols:\n",
    "            if len(df1[col].shape) == 1: # for 1D arrays (usally y, target variables)\n",
    "                # define label encoder\n",
    "                encoder = preprocessing.LabelEncoder()\n",
    "                # create new columns and preserve the original columns\n",
    "                dataframe.loc[:,col+'_encoded'] = encoder.fit_transform(dataframe[col])\n",
    "            else: # for 2D arrays (usually X, input features)\n",
    "                # define ordinal encoder\n",
    "                encoder = preprocessing.LabelEncoder()\n",
    "                # create new columns and preserve the original columns\n",
    "                dataframe.loc[:,col+'_encoded'] = encoder.fit_transform(dataframe[col])\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6f48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataset(dataframe, \n",
    "                       X_col: str = 'X', \n",
    "                       y_col: str = 'y', \n",
    "                       w_col: str = None ,\n",
    "                       id_col: str = None, \n",
    "                       return_csv = False):\n",
    "    \"\"\"\n",
    "    Converts dataframe into a deepchem dataset object.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - dataframe: featurized dataframe\n",
    "    - X_col: (str or List[str]) name(s) of the column(s) containing the X array.\n",
    "    - y_col: (str or List[str]) name(s) of the column(s) containing the y array.\n",
    "    - w_col: (str or List[str]) name(s) of the column(s) containing the w array.\n",
    "    - id_col: (str) name of the column containing the ids.\n",
    "    - return_csv: (True/False) whether a viewable csv of the data will be returned with the dataset object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - deepchem dataset\n",
    "    - (conditional) csv of dataframe\n",
    "    \"\"\"\n",
    "    # define x\n",
    "    X = dataframe[X_col]\n",
    "    \n",
    "    # define y\n",
    "    y = dataframe[y_col]\n",
    "    \n",
    "    # define weight\n",
    "    if w_col is not None:\n",
    "        w = dataframe[w_col]\n",
    "    else:\n",
    "        w = None\n",
    "    \n",
    "    # define ids\n",
    "    if id_col is not None:\n",
    "        ids = dataframe[id_col]\n",
    "    else:\n",
    "        ids = None\n",
    "    \n",
    "    # create deepchem dataset object\n",
    "    to_dataset = dc.data.NumpyDataset(X, y, w, ids)\n",
    "    dataset = to_dataset.from_dataframe(dataframe)\n",
    "    \n",
    "    # return dataset and csv of current dataframe if return_csv equals True\n",
    "    # return only dataset if return_csv equals False or unspecified\n",
    "    if return_csv is True:\n",
    "        csv = dataframe.to_csv(index = True, header=True)\n",
    "        return dataset, csv\n",
    "    else:\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f61b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformation(dataset, \n",
    "                        transformations: list = ['NormalizationTransformer'], \n",
    "                        to_transform: list = [],  \n",
    "                        **kwargs):\n",
    "    \"\"\"\n",
    "    Transforms and splits deepchem dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - dataset: deepchem dataset to be transformed\n",
    "    - transformations: (List[str]) list of transformation methods to pass dataset through\n",
    "    - to_transform: (list[str]) list of elements to transform, and can include 'X', 'y', or 'w'\n",
    "    - **kwargs: keyword arguments to be passed to the selected transformer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Transformed dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # feed dataset into list of transformers sequentially, returning a single transformed dataset\n",
    "    for transformation in transformations:\n",
    "        if to_transform is not None:\n",
    "        \n",
    "            if(all(elem in to_transform for elem in ['X', 'y', 'w'])):\n",
    "                transformer = getattr(dc.trans, transformation)(transform_X=True, transform_y=True, transform_w=True, dataset=dataset, **kwargs)\n",
    "            elif(all(elem in to_transform for elem in ['X', 'y'])):\n",
    "                transformer = getattr(dc.trans, transformation)(transform_X=True, transform_y=True, dataset=dataset, **kwargs)\n",
    "            elif 'X' in to_transform:\n",
    "                transformer = getattr(dc.trans, transformation)(transform_X=True, dataset=dataset, **kwargs)\n",
    "            elif 'y' in to_transform:\n",
    "                transformer = getattr(dc.trans, transformation)(transform_y=True, dataset=dataset, **kwargs)\n",
    "            else:\n",
    "                transformer = getattr(dc.trans, transformation)(dataset=dataset, **kwargs)\n",
    "        else:\n",
    "            transformer = getattr(dc.trans, transformation)(dataset=dataset, **kwargs)\n",
    "            \n",
    "        dataset = transformer.transform(dataset)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "93c5ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitting(dataset, \n",
    "                   splitter: str = 'RandomSplitter', \n",
    "                   split_type: str = 'train_valid_test_split', \n",
    "                   **kwargs):\n",
    "    \"\"\"\n",
    "    Transforms and splits deepchem dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - dataset: deepchem dataset to be split\n",
    "    - splitter: (str) class of deepchem split method\n",
    "    - split_type: (str) type of split (k_fold_split/train_test_split/train_valid_test_split)\n",
    "    - **kwargs: keyword arguments to be passed to the selected splitter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Split dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # split data\n",
    "    data_splitter = getattr(dc.splits, splitter)\n",
    "    \n",
    "    if hasattr(data_splitter, __init__):\n",
    "        data_splitter = data_splitter(**kwargs)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # this only allows the following three split_types to be used\n",
    "    # the 'split' option is excluded since it seems to do the same thing as 'train_valid_test_split' but returns non-dataset objects\n",
    "    # might want to write code to reset certain defaults depending on which split_type is chosen (i.e. from None to 0.8)\n",
    "    \n",
    "    if split_type == 'k_fold_split' or split_type == 'k':\n",
    "        data_split = data_splitter.k_fold_split(dataset, **kwargs)\n",
    "    elif split_type == 'train_test_split' or split_type == 'tt':\n",
    "        data_split = data_splitter.train_test_split(dataset, **kwargs)\n",
    "    elif split_type == 'train_valid_test_split' or split_type =='tvt':\n",
    "        data_split = data_splitter.train_valid_test_split(dataset=dataset, **kwargs)\n",
    "    elif split_type == 'generate_scaffolds':\n",
    "        if hasattr(data_splitter, 'generate_scaffolds'):\n",
    "            data_split = data_splitter.generate_scaffolds(dataset, **kwargs) # Unsure about functionality, code may need to be added\n",
    "        else:\n",
    "            raise AttributeError ('split_type may only be set as generate_scaffolds if splitter set as ScaffoldSplitter')\n",
    "    else: \n",
    "        print('split_type string is not a recognized split') # should change this to raise an error of some kind\n",
    "    \n",
    "    return data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cfd638e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a version to use which works with the RandomSplitter \n",
    "that we can use for testing while I fix the bugs in the correct version.\n",
    "\"\"\"\n",
    "\n",
    "def data_splitting(dataset,  \n",
    "                   split_type: str = 'train_valid_test_split', \n",
    "                   k: int = None, \n",
    "                   frac_train: float = None, \n",
    "                   frac_valid: float = None, \n",
    "                   frac_test: float = None, \n",
    "                   log_every_n: int = None):\n",
    "    \"\"\"\n",
    "    Transforms and splits deepchem dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - dataset: deepchem dataset to be split\n",
    "    - splitter: (str) class of deepchem split method\n",
    "    - split_type: (str) type of split (k_fold_split/train_test_split/train_valid_test_split)\n",
    "    - k: int\n",
    "    - frac_train: float \n",
    "    - frac_valid: float \n",
    "    - frac_test: float \n",
    "    - log_every_n: int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Split dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # split data\n",
    "    #data_splitter = getattr(dc.splits, splitter)\n",
    "    data_splitter = dc.splits.RandomSplitter()\n",
    "\n",
    "    # this only allows the following three split_types to be used\n",
    "    # the 'split' option is excluded since it seems to do the same thing as 'train_valid_test_split' but returns non-dataset objects\n",
    "    \n",
    "    if split_type == 'k_fold_split' or split_type == 'k':\n",
    "        data_split = data_splitter.k_fold_split(dataset=dataset, k=k)\n",
    "    elif split_type == 'train_test_split' or split_type == 'tt':\n",
    "        data_split = data_splitter.train_test_split(dataset=dataset, frac_train=frac_train)\n",
    "    elif split_type == 'train_valid_test_split' or split_type =='tvt':\n",
    "        data_split = data_splitter.train_valid_test_split(dataset=dataset, frac_train=frac_train, frac_valid=frac_valid, frac_test=frac_test)\n",
    "    elif split_type == 'generate_scaffolds':\n",
    "        if hasattr(data_splitter, 'generate_scaffolds'):\n",
    "            data_split = data_splitter.generate_scaffolds(dataset, log_every_n) # Unsure about functionality, code may need to be added\n",
    "        else:\n",
    "            raise AttributeError ('split_type may only be set as generate_scaffolds if splitter set as ScaffoldSplitter')\n",
    "    else: \n",
    "        print('split_type string is not a recognized split') # should change this to raise an error of some kind\n",
    "    \n",
    "    return data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c0d8f1",
   "metadata": {},
   "source": [
    "## Test dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29dbdf6",
   "metadata": {},
   "source": [
    "### Create new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f47f9dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id    Make    Model  Price  Engine_HP   Color\n",
      "0   1   Honda    Civic  22000      150.5    Blue\n",
      "1   2  Toyota  Corolla  25000      220.0   Black\n",
      "2   3    Ford    Focus  27000      124.5    Blue\n",
      "3   4    Audi       A4  35000      190.0    Pink\n",
      "4   5  Subaru   Legacy  90000      120.0  Orange\n",
      "5   6  Subaru  Outback  10000      124.5   Green\n",
      "6   7    Ford   Fiesta  23000      185.5    Pink\n",
      "7   8    Jeep    Grand  27000      190.0   Black\n"
     ]
    }
   ],
   "source": [
    "cars = {'id': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        'Make': ['Honda','Toyota','Ford','Audi','Subaru','Subaru','Ford','Jeep'],\n",
    "        'Model': ['Civic','Corolla','Focus','A4','Legacy','Outback','Fiesta','Grand'],\n",
    "        'Price': [22000,25000,27000,35000,90000,10000,23000,27000],\n",
    "        'Engine_HP': [150.5, 220.0, 124.5, 190.0, 120.0, 124.5, 185.5, 190.0],\n",
    "        'Color': ['Blue','Black','Blue','Pink','Orange','Green','Pink','Black']\n",
    "        }\n",
    "\n",
    "df1 = pd.DataFrame(cars, columns = ['id', 'Make', 'Model', 'Price', 'Engine_HP', 'Color'])\n",
    "\n",
    "print (df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e9efb",
   "metadata": {},
   "source": [
    "### Create categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b1d006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = convert_to_categorical(dataframe = df1, cols = ['Make', 'Model', 'Color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dbf526",
   "metadata": {},
   "source": [
    "#### Print new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2755c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Make</th>\n",
       "      <th>Model</th>\n",
       "      <th>Price</th>\n",
       "      <th>Engine_HP</th>\n",
       "      <th>Color</th>\n",
       "      <th>Make_encoded</th>\n",
       "      <th>Model_encoded</th>\n",
       "      <th>Color_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Civic</td>\n",
       "      <td>22000</td>\n",
       "      <td>150.5</td>\n",
       "      <td>Blue</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Corolla</td>\n",
       "      <td>25000</td>\n",
       "      <td>220.0</td>\n",
       "      <td>Black</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Focus</td>\n",
       "      <td>27000</td>\n",
       "      <td>124.5</td>\n",
       "      <td>Blue</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Audi</td>\n",
       "      <td>A4</td>\n",
       "      <td>35000</td>\n",
       "      <td>190.0</td>\n",
       "      <td>Pink</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Subaru</td>\n",
       "      <td>Legacy</td>\n",
       "      <td>90000</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Orange</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Subaru</td>\n",
       "      <td>Outback</td>\n",
       "      <td>10000</td>\n",
       "      <td>124.5</td>\n",
       "      <td>Green</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Fiesta</td>\n",
       "      <td>23000</td>\n",
       "      <td>185.5</td>\n",
       "      <td>Pink</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>Grand</td>\n",
       "      <td>27000</td>\n",
       "      <td>190.0</td>\n",
       "      <td>Black</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    Make    Model  Price  Engine_HP   Color  Make_encoded  Model_encoded  \\\n",
       "0   1   Honda    Civic  22000      150.5    Blue             2              1   \n",
       "1   2  Toyota  Corolla  25000      220.0   Black             5              2   \n",
       "2   3    Ford    Focus  27000      124.5    Blue             1              4   \n",
       "3   4    Audi       A4  35000      190.0    Pink             0              0   \n",
       "4   5  Subaru   Legacy  90000      120.0  Orange             4              6   \n",
       "5   6  Subaru  Outback  10000      124.5   Green             4              7   \n",
       "6   7    Ford   Fiesta  23000      185.5    Pink             1              3   \n",
       "7   8    Jeep    Grand  27000      190.0   Black             3              5   \n",
       "\n",
       "   Color_encoded  \n",
       "0              1  \n",
       "1              0  \n",
       "2              1  \n",
       "3              4  \n",
       "4              3  \n",
       "5              2  \n",
       "6              4  \n",
       "7              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3cc21c",
   "metadata": {},
   "source": [
    "### Convert to deepchem dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af0030f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cars = convert_to_dataset(dataframe=df2, X_col='Engine_HP', y_col='Price', w_col=None, id_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e427bc",
   "metadata": {},
   "source": [
    "#### Print dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0563debb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NumpyDataset X.shape: (8, 0), y.shape: (8, 0), w.shape: (8, 0), ids: [0 1 2 3 4 5 6 7], task_names: []>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e001dd1",
   "metadata": {},
   "source": [
    "### Transform and split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c601335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NumpyDataset X.shape: (8, 0), y.shape: (8, 0), w.shape: (8, 0), ids: [0 1 2 3 4 5 6 7], task_names: []>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data = data_transformation(dataset_cars, \n",
    "                        transformations = ['NormalizationTransformer', 'MinMaxTransformer'], \n",
    "                        to_transform = ['X'])\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3916979b",
   "metadata": {},
   "source": [
    "### Split dataset\n",
    "This version of data splitting only works for RandomSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5f1eb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<NumpyDataset X.shape: (4, 0), y.shape: (4, 0), w.shape: (4, 0), ids: [0 5 2 7], task_names: []>,\n",
       " <NumpyDataset X.shape: (2, 0), y.shape: (2, 0), w.shape: (2, 0), ids: [3 1], task_names: []>,\n",
       " <NumpyDataset X.shape: (2, 0), y.shape: (2, 0), w.shape: (2, 0), ids: [6 4], task_names: []>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splittest = dc.splits.RandomSplitter()\n",
    "test_splitting = splittest.train_valid_test_split(dataset = transformed_data, frac_train = 0.5, frac_valid = 0.25, frac_test = 0.25)\n",
    "test_splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789bebed",
   "metadata": {},
   "source": [
    "# Unused Fuctions\n",
    "#### These functions are either redundant or incompatible with the rest of the current package code, but the functions themselves are still independently functional and can be used for other purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eabd8bb",
   "metadata": {},
   "source": [
    "### Unused categorical converter\n",
    "This function converts categorical columns to integers using the Pandas get_dummies function (similar to OneHotEncoding).  It was removed from used given that part of its functionality (assigning each category its own column) is made redundant by the modeling functions downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_categorical(dataframe, cols=None):\n",
    "    \"\"\"\n",
    "    Converts non-numerical categorical values to integers.  This function is most useful for nominal variables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - dataframe: featurized dataframe\n",
    "    - cols: list of categorical columns to convert to integers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - modified dataframe with user selected columns with categorical string values converted to integer values\n",
    "    - list of list containing new column names separated by originating column\n",
    "    \"\"\"\n",
    "    \n",
    "    # create list if new column prefixes\n",
    "    prefix_list = []\n",
    "    count = 0\n",
    "    for col in cols:\n",
    "        count = count + 1\n",
    "        prefix_list.append('categorical_column'+str(count))\n",
    "        \n",
    "    # create dict to link original column names to newly created prefixes\n",
    "    new_column_dict = dict(zip(cols, prefix_list))\n",
    "    \n",
    "    # generate binary values using get_dummies\n",
    "    if cols is not None:\n",
    "        if type(cols) == str: # allow single columns to be input as strings\n",
    "            cols = [cols]\n",
    "            dataframe = pd.get_dummies(dataframe, prefix=prefix_list, columns=cols)\n",
    "        elif type(cols) == list:\n",
    "            dataframe = pd.get_dummies(dataframe, prefix=prefix_list, columns=cols)\n",
    "        \n",
    "        # get list of new categorical columns\n",
    "        categories_list = []\n",
    "        for col in cols:\n",
    "            var_list_name = col\n",
    "            var_list_name = []\n",
    "            for column_name in list(dataframe):\n",
    "                if new_column_dict[col] in column_name:\n",
    "                    var_list_name.append(column_name)\n",
    "                else:\n",
    "                    continue\n",
    "            categories_list.append(var_list_name)\n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return dataframe, categories_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
